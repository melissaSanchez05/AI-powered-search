{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyISr3c6L8cY"
      },
      "source": [
        "# Adversarial Search: Playing \"Mean\" Connect 4\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Name: Melissa Sanchez\n",
        "\n",
        "I understand that my submission needs to be my own work: Yes\n",
        "\n",
        "Points: 10\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You will implement different versions of agents that play \"Mean\" Connect 4:\n",
        "\n",
        "> \"Connect 4 is a two-player connection board game, in which the players choose a color and then take turns dropping colored discs into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.\" (see [Connect Four on Wikipedia](https://en.wikipedia.org/wiki/Connect_Four))\n",
        "\n",
        "> **The mean part:** This game has an additional rule. Every time it is a player's turn, the player can decide to instead of playing a new disk, take a bottom row disk of the opponent and place it back in the top of the same column. All disks above the removed disk will fall down one position and the removed one will be placed on top. Note that a player can only move an _opponent's disc_ that is in the _bottom row_ of the board. **Further, you are not allowed to play a mean move if your opponent just played one.** This ensures the game will end at some point. This also may affect the definition of a state, compared with standard Connect 4.\n",
        "\n",
        "If a mean move causes both players to win, the game immediately ends and it is a tie, even if one player has more connect-4s than the other one. If a mean move causes one player to win, then the game also ends and the player with the connect-4 is the winner.\n",
        "\n",
        "Note that normal [Connect-4 has been solved](https://en.wikipedia.org/wiki/Connect_Four#Mathematical_solution)\n",
        "in 1988. A connect-4 solver with a discussion of how to solve different parts of the problem can be found here: https://connect4.gamesolver.org/en/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuWbcK6gL8cZ"
      },
      "source": [
        "## Task 1: Defining the Search Problem [1 point]\n",
        "\n",
        "Define the components of the search problem associated with this game:\n",
        "\n",
        "* Initial state\n",
        "* Actions\n",
        "* Transition model\n",
        "* Test for the terminal state\n",
        "* Utility for terminal states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzLitKOdL8cf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "initial state: a board with 6 rows and 7 columns with no disks\n",
        "actions: players can\n",
        "          1) select a disk of thier color and drop it along the columns\n",
        "          2) play a mean move: select a botton row disk from the opponent and place it back in the top of the same column\n",
        "          3) a mean move can't be played if the opponent just played one\n",
        "transition model:\n",
        "                  1) switch player's turns\n",
        "                  2) update the board by the current player move\n",
        "                  3) update a flag to indicate if a mean move was played\n",
        "                  4) check the board for a win or draw\n",
        "test for the terminal state: a player as\n",
        "                              1) check the board for a winner: form a 4-line vertical, horizontal, or diagonal of the same color disk\n",
        "                              2) board has been compleated: no more disks can be added\n",
        "utility for terminal states:\n",
        "                            1) o for a tie\n",
        "                            2) 1 for a win by the player\n",
        "                            3) -1 for a win by the opponent\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHDORt8_L8cg"
      },
      "source": [
        "How big is the state space? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_GIsO80L8cg"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "standard connect-4 has a spece of 3(tie, p1,p2) ** (7 * 6)= 3 ** 42, however in the addition of the mean move\n",
        "the space is larger compared to the standard connect-4. However the legal posiiton of both the standard game\n",
        "and modified are the same\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ci9AS2QL8ch"
      },
      "source": [
        "How big is the game tree that minimax search will go through? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTkWu4ZiL8ci"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "the game tree is all posible moves that can be made by one player. Each player can place a disk in one of the each column (7) plus\n",
        "if a mean move is allowed it could be 7 more posible places, 14 per turn.\n",
        "Each game could take up to 42 moves (to filled the whole board).\n",
        "On the worse case the tree can go up to 14 ** 42 nodes\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u66LNfUEL8ci"
      },
      "source": [
        "## Task 2: Game Environment and Random Agent [3 point]\n",
        "\n",
        "You can use a numpy character array as the board. Note that the following function can create boards of different sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gFoRCZefL8cj",
        "outputId": "2ddaf2f3-c21c-489b-d28a-0848274ec275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def empty_board(shape=(6, 7)):\n",
        "    return np.full(shape=shape, fill_value=0)\n",
        "\n",
        "print(empty_board())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCCwVya8L8cj"
      },
      "source": [
        "Instead of colors (red and yellow), you can use 1 and -1 to represent the players Max and Min. Make sure that your agent functions all have the from: `agent_type(state, player = 1)`, where board is the current board position and player is the player (1, -1) whose next move it is and who the agent should play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtsGKQPsL8ck"
      },
      "source": [
        "Visualization code by Randolph Rankin:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "dRChZkdZL8ck",
        "outputId": "1148a38a-2cc7-48ec-8dd0-b1df0714d6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize(board):\n",
        "    plt.axes()\n",
        "    rectangle=plt.Rectangle((-0.5,len(board)*-1+0.5),len(board[0]),len(board),fc='blue')\n",
        "    circles=[]\n",
        "    for i,row in enumerate(board):\n",
        "        for j,val in enumerate(row):\n",
        "            color='white' if val==0 else 'red' if val==1 else 'yellow'\n",
        "            circles.append(plt.Circle((j,i*-1),0.4,fc=color))\n",
        "\n",
        "    plt.gca().add_patch(rectangle)\n",
        "    for circle in circles:\n",
        "        plt.gca().add_patch(circle)\n",
        "\n",
        "    plt.axis('scaled')\n",
        "    plt.show()\n",
        "\n",
        "board = [[0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 1, 0, 0, 0],\n",
        "         [0, 0, 0, 1, 0, 0, 0],\n",
        "         [0,-1,-1, 1,-1, 0, 0]]\n",
        "\n",
        "visualize(board)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOa_fihRL8ck"
      },
      "source": [
        "Implement helper functions for:\n",
        "\n",
        "* The transition model $result(s, a)$.\n",
        "* The utility function $utility(s)$.\n",
        "* Check for terminal states $terminal(s)$.\n",
        "* A check for available actions in each state $actions(s)$.\n",
        "\n",
        "Make sure that all these functions work with boards of different sizes (number of columns and rows)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8sDMBWFtL8ck"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def check_board(board, player, connect=4):\n",
        "  row, column = board.shape\n",
        "  #horizontal\n",
        "  for r in range(row):\n",
        "    for c in range(column - connect + 1):\n",
        "      if np.all(board[r,c : c + connect] == player):\n",
        "        return True\n",
        "  #vertical\n",
        "  for c in range(column):\n",
        "    for r in range(row - connect + 1):\n",
        "      if np.all(board[r: r + connect, c] == player):\n",
        "        return True\n",
        "  #diagonal /\n",
        "  #reduced the grid from row and column up to possible moves up to connect on the lower left corner\n",
        "  for r in range(connect - 1, row):\n",
        "    for c in range(column - connect + 1):\n",
        "      if np.all(np.array([board[r - i , c + i] for i in range(connect)]) == player):\n",
        "        return True\n",
        "\n",
        "  #diagonal \\\n",
        "  #reduced the grid from row and column up to possible moves up to connect on the upper left corner\n",
        "  for r in range(row - connect + 1):\n",
        "    for c in range(column - connect + 1):\n",
        "      if np.all(np.array([board[r + i , c + i] for i in range(connect)]) == player):\n",
        "        return True\n",
        "\n",
        "  return False\n",
        "\n",
        "\n",
        "def utility(state): #determinate a winnder or tie\n",
        "  board, mean_flag, player = state\n",
        "\n",
        "  #check if current player is a winner\n",
        "  current_player_result = check_board(board, player)\n",
        "  opponent_player_result = check_board(board, -player)\n",
        "\n",
        "  if current_player_result and opponent_player_result:\n",
        "    return 0 #tie\n",
        "  elif current_player_result:\n",
        "    return player\n",
        "  elif opponent_player_result:\n",
        "    return -player\n",
        "  else:\n",
        "    return 0 #board is full\n",
        "\n",
        "def terminal(state): #check for a winner\n",
        "  board, mean_flag, player = state\n",
        "\n",
        "  #check if the game has terminated (any player has a connect or board is full)\n",
        "  current_player_result = check_board(board, player)\n",
        "  opponent_player_result = check_board(board, -player)\n",
        "  if current_player_result or opponent_player_result or not (board == 0).any():\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def actions(state): #actions\n",
        "  #return all posible actions based on the board's state\n",
        "  # actions are a tuple (move_type, c) move type is either standard or mean in which column\n",
        "  #standard move is posible at any column that is not full\n",
        "  #mean move is posible if 1) mean_flag is not set and 2) there is an opponent disk in the bottom row\n",
        "  board, mean_flag  , player= state\n",
        "  row, column = board.shape\n",
        "\n",
        "  moves = []\n",
        "\n",
        "  #standard moves\n",
        "  for c in range(column):\n",
        "    if board[0, c] == 0:\n",
        "      moves.append(('standard', c ))\n",
        "\n",
        "  #mean moves\n",
        "  if not mean_flag:\n",
        "    for c in range(column):\n",
        "      if board[row - 1, c] == (-1 * player):\n",
        "        moves.append(('mean', c))\n",
        "\n",
        "  return moves\n",
        "\n",
        "def result_state(state, action): #transitional model\n",
        "  board, mean_flag , player = state\n",
        "  row, column = board.shape\n",
        "  new_board = board.copy()\n",
        "  move_type, c = action\n",
        "\n",
        "  #play a move by the current player\n",
        "  #update mean_flag is mean move was played\n",
        "\n",
        "  if move_type == 'standard':\n",
        "    #drop a disk\n",
        "    for r in range(row - 1, -1, -1):\n",
        "      if new_board[r, c] == 0:\n",
        "        new_board[r,c] = player\n",
        "        break\n",
        "    #update state\n",
        "    new_state = (new_board, False, -player)\n",
        "\n",
        "  elif move_type == 'mean' and new_board[row -1, c] == -player:#play a mean move\n",
        "\n",
        "    #shift by 1\n",
        "    for r in range(row - 1, 0, -1):\n",
        "      new_board[r,c] = new_board[r-1,c]\n",
        "\n",
        "    #add the opponent player disk on top\n",
        "    placed = False\n",
        "    for r in range(row):\n",
        "      if new_board[r,c] == 0:\n",
        "        new_board[r,c] = -player\n",
        "        placed = True\n",
        "        break\n",
        "    #overide the top cell\n",
        "    if not placed:\n",
        "      new_board[0,c] = -player\n",
        "    new_state = (new_board, True, -player)\n",
        "  else:\n",
        "    new_state = (new_board, mean_flag, -player)\n",
        "  return new_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T7SYQM4L8cl"
      },
      "source": [
        "Implement an agent that plays randomly. Make sure the agent function receives as the percept the game state and returns a valid action. Use an agent function definition with the following signature (arguments):\n",
        "\n",
        "`def random_player(state, player = None): ...`\n",
        "\n",
        "The argument `player` is used for agents that do not store what side they are playing. The value passed on by the environment should be 1 ot -1 for playerred and yellow, respectively.  See [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g0uUKNKNL8cl"
      },
      "outputs": [],
      "source": [
        "def random_player(state, player=None):\n",
        "  if player is not None:\n",
        "      board, mean_flag, _ = state\n",
        "      state = (board, mean_flag, player)\n",
        "\n",
        "  legal_moves = actions(state)\n",
        "\n",
        "  if not legal_moves:\n",
        "    return None\n",
        "\n",
        "  return random.choice(legal_moves)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS3yOFlbL8cl"
      },
      "source": [
        "Let two random agents play against each other 1000 times. Look at the [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) to see how the environment uses the agent functions to play against each other.\n",
        "\n",
        "How often does each player win? Is the result expected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": [],
        "id": "55tvNH3vL8cl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4869bdac-aa97-49b1-d8cb-cb7eca6f7b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For 1000 games\n",
            "player 1:  won 470 times\n",
            "player -1: won 285 times\n",
            "player resulted in at tie:  245 times\n"
          ]
        }
      ],
      "source": [
        "def start_game(random_player, row, column):\n",
        "\n",
        "  board = empty_board((row, column))\n",
        "\n",
        "  state = (board, False, 1 )\n",
        "\n",
        "  #play until the terimal state\n",
        "  while not terminal(state):\n",
        "    current_player = state[2]\n",
        "\n",
        "    action = random_player(state, current_player)\n",
        "\n",
        "    if action is None:\n",
        "      break\n",
        "\n",
        "    state = result_state(state, action)\n",
        "  return utility(state)\n",
        "\n",
        "games = 1000\n",
        "r, c = 6,7\n",
        "results = {1: 0, -1: 0, 0: 0}\n",
        "for _ in range(games):\n",
        "  winner = start_game(random_player, r, c)\n",
        "  results[winner]+=1\n",
        "\n",
        "print(f'For {games} games')\n",
        "print(f'player 1:  won {results[1]} times')\n",
        "print(f'player -1: won {results[-1]} times')\n",
        "print(f'player resulted in at tie:  {results[0]} times')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvzOxRDpL8cm"
      },
      "source": [
        "## Task 3: Minimax Search with Alpha-Beta Pruning [3 points]\n",
        "\n",
        "### Implement the search starting.\n",
        "\n",
        "Implement the search starting from a given state and specifying the player and put it into an agent function.\n",
        "You can use code from the [tic-tac-toe example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_alpha_beta_tree_search.ipynb).\n",
        "\n",
        "__Notes:__\n",
        "* Make sure that all your agent functions have a signature consistent with the random agent above.\n",
        "* The search space for a $6 \\times 7$ board is large. You can experiment with smaller boards (the smallest is $4 \\times 4$) and/or changing the winning rule to connect 3 instead of 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GJ2xQLrWL8cm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def min_max_value(state, alpha, beta, depth, max_player):\n",
        "  #alpha: best exlpored option for the minimizer\n",
        "  #beta: best explored option for the maximizer\n",
        "  #depth: search depth\n",
        "  #max_player: original player maximizer\n",
        "  board, mean_flag, player = state\n",
        "\n",
        "  #if the game is over of depth is 0\n",
        "  if terminal(state) or depth == 0:\n",
        "    return utility(state)\n",
        "\n",
        "  #maximizing player (original player)\n",
        "  if state[2] == max_player:\n",
        "    value = float('-inf')\n",
        "    for action in actions(state):\n",
        "      child_state = result_state(state, action)\n",
        "      value = max(value, min_max_value(child_state, alpha, beta, depth - 1, max_player))\n",
        "      alpha = max(alpha, value)\n",
        "      if alpha >= beta:\n",
        "        break\n",
        "    return value\n",
        "  #minimzing player (opponent player)\n",
        "  else:\n",
        "    value = float('inf')\n",
        "    for action in actions(state):\n",
        "      child_state = result_state(state, action)\n",
        "      value = min(value, min_max_value(child_state, alpha, beta, depth - 1, max_player))\n",
        "      alpha = min(beta, value)\n",
        "      if beta <= alpha:\n",
        "        break\n",
        "    return value\n",
        "\n",
        "def min_max_agent(state, player=None, depth_limit=4):\n",
        "  if player is None:\n",
        "    board, mean_flag, _ = state\n",
        "    state = (board, mean_flag, player)\n",
        "\n",
        "  max_player = state[2]\n",
        "  best_action = None\n",
        "  best_value = float('-inf')\n",
        "\n",
        "  for action in actions(state):\n",
        "    child_state = result_state(state, action)\n",
        "    value = min_max_value(child_state, float('-inf'), float('inf'), depth_limit - 1, max_player)\n",
        "\n",
        "    if value > best_value:\n",
        "      best_value = value\n",
        "      best_action = action\n",
        "  return best_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQeK9qFXL8cm"
      },
      "source": [
        "Experiment with some manually created boards (at least 5) to check if the agent spots winning opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ECcMh7WTL8cm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6242b61c-838e-4e50-c577-101b4905ff94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    (r,c)  move selected  time (s)\n",
            "0  (6, 4)  (standard, 0)  0.641659\n",
            "1  (6, 5)  (standard, 0)  1.170766\n",
            "2  (6, 6)  (standard, 0)  3.882929\n",
            "3  (6, 7)  (standard, 0)  0.013358\n",
            "4  (6, 8)  (standard, 0)  0.018073\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "def winning_board(row, col, player):\n",
        "  board = empty_board((row, col))\n",
        "  if col >4:\n",
        "    board[row -1 : col - 1] = player\n",
        "  return board\n",
        "\n",
        "\n",
        "times = []\n",
        "legal_moves =[]\n",
        "diamentions =[]\n",
        "for c in [4,5,6,7,8]:\n",
        "  board = winning_board(6, c, player=1)\n",
        "  state = (board, False, 1)\n",
        "  start = time.time()\n",
        "  move = min_max_agent(state, player=1, depth_limit=4)\n",
        "  end = time.time() - start\n",
        "\n",
        "  times.append(end)\n",
        "  legal_moves.append(move)\n",
        "  diamentions.append((6,c))\n",
        "\n",
        "data = {\n",
        "  '(r,c)': diamentions,\n",
        "  'move selected': legal_moves,\n",
        "  'time (s)': times\n",
        "}\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ByI_b2VL8cm"
      },
      "source": [
        "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1kG2qLFIL8cm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "870d4987-d6ac-4630-b8d4-b9ed2fac9b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    (r,c)  move selected  time (s)\n",
            "0  (6, 4)  (standard, 3)  0.310393\n",
            "1  (6, 5)  (standard, 0)  0.003353\n",
            "2  (6, 6)  (standard, 0)  0.006651\n",
            "3  (6, 7)  (standard, 0)  0.008199\n",
            "4  (6, 8)  (standard, 0)  0.014637\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "def manual_boards(col):\n",
        "  board = empty_board((6,col))\n",
        "  board[5, :col -1 ] = 1\n",
        "  return board\n",
        "\n",
        "times = []\n",
        "legal_moves =[]\n",
        "diamentions =[]\n",
        "for col in [4,5,6,7,8]:\n",
        "  board = manual_boards(col)\n",
        "  state = (board, False, 1)\n",
        "\n",
        "  start = time.time()\n",
        "  move = min_max_agent(state, player=1, depth_limit=4)\n",
        "  end = time.time() - start\n",
        "\n",
        "  times.append(end)\n",
        "  legal_moves.append(move)\n",
        "  diamentions.append((6,col))\n",
        "\n",
        "data = {\n",
        "'(r,c)': diamentions,\n",
        "'move selected': legal_moves,\n",
        "'time (s)': times\n",
        "}\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDHTe0LcL8cn"
      },
      "source": [
        "### Move ordering\n",
        "\n",
        "Starting the search with better moves will increase the efficiency of alpha-beta pruning. Describe and implement a simple move ordering strategy. Make a table that shows how the ordering strategies influence the time it takes to make a move."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "omeYo7HyL8cn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f37c50d1-9b49-45e1-ab33-70447d4dc0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   number of columns  standard min max  move ordering\n",
            "0                  4          0.336823       0.336823\n",
            "1                  5          1.311106       1.311106\n",
            "2                  6          3.560985       3.560985\n",
            "3                  7          7.779842       7.779842\n",
            "4                  8         15.464889      15.464889\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "by dropping disks in the center of the board the probablity of forming a 4-disk is greater.\n",
        "1)prioritize the best move first (the center of the board)\n",
        "2)organize legal moves by the distance to the center of the board\n",
        "'''\n",
        "import pandas as pd\n",
        "def move_ordering(moves, board):\n",
        "  rows, column = board.shape\n",
        "  center = column // 2\n",
        "  return sorted(moves, key=lambda x : abs(x[1] - center))\n",
        "\n",
        "def min_max_agent_ordering(state, player=None, depth_limit=4):\n",
        "  if player is None:\n",
        "      board, mean_flag, _ = state\n",
        "      state = (board, mean_flag, player)\n",
        "\n",
        "  max_player = state[2]\n",
        "  best_action = None\n",
        "  best_value = float('-inf')\n",
        "  ordering = move_ordering(actions(state), state[0])\n",
        "  for action in ordering:\n",
        "    child_state = result_state(state, action)\n",
        "    value = min_max_value(child_state, float('-inf'), float('inf'), depth_limit - 1, max_player)\n",
        "\n",
        "    if value > best_value:\n",
        "      best_value = value\n",
        "      best_action = action\n",
        "  return best_action\n",
        "\n",
        "\n",
        "def display_results():\n",
        "  columns =[]\n",
        "  ordering = []\n",
        "  standard = []\n",
        "  trials = 10\n",
        "  for col in [4,5,6,7,8]:\n",
        "    board = empty_board((6,col))\n",
        "    state = (board, False, 1)\n",
        "\n",
        "    time_standard =[]\n",
        "    #standard min max agent\n",
        "    for _ in range(trials):\n",
        "      start = time.time()\n",
        "      move = min_max_agent(state, player=1, depth_limit=4)\n",
        "      end = time.time() - start\n",
        "      time_standard.append(end)\n",
        "\n",
        "    avg_time_standard = sum(time_standard) / len(time_standard)\n",
        "\n",
        "    #ordering min max agent\n",
        "    time_ordering = []\n",
        "    for _ in range(trials):\n",
        "      start = time.time()\n",
        "      move = min_max_agent_ordering(state, player=1, depth_limit=4)\n",
        "      end = time.time() - start\n",
        "      time_ordering.append(end)\n",
        "\n",
        "    avg_time_ordering = sum(time_ordering) / len(time_ordering)\n",
        "\n",
        "    columns.append(col)\n",
        "    standard.append(avg_time_standard)\n",
        "    ordering.append(avg_time_ordering)\n",
        "\n",
        "  data = {\n",
        "        'number of columns': columns,\n",
        "        'standard min max': standard,\n",
        "        'move ordering': standard\n",
        "    }\n",
        "  df = pd.DataFrame.from_dict(data)\n",
        "  print(df)\n",
        "display_results()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ZJDd9nL8cn"
      },
      "source": [
        "### The first few moves\n",
        "\n",
        "Start with an empty board. This is the worst case scenario for minimax search with alpha-beta pruning since it needs solve all possible games that can be played (minus some pruning) before making the decision. What can you do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip9dbCRgL8cn"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "worse-case: emtpy board\n",
        "\n",
        "-> whenteh board is empty the branching is big since it needs to account for all posiblities and min max even with alpha-beta\n",
        "    purining still needs to check all posiblities\n",
        "\n",
        "what do to:\n",
        "1) have a set of best initial moves such as moves in the center\n",
        "2) be stratigic about steps, only allow moves that will benefic the player\n",
        "3) use the best move first\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhk1YRnDL8cn"
      },
      "source": [
        "### Playtime\n",
        "\n",
        "Let the Minimax Search agent play a random agent on a small board. Analyze wins, losses and draws."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Kx1qffv2L8co",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ad00badf-3f07-45cc-8751-0bb1aaaf1eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For 1000 games\n",
            "player 1:  won 559 times\n",
            "player -1: won 7 times\n",
            "player resulted in at tie:  434 times\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def play_time( board_shape=(4,4), depth_limit=4):\n",
        "  board = empty_board(board_shape)\n",
        "  state = (board, False, 1)\n",
        "\n",
        "  while not terminal(state):\n",
        "    current_player = state[2]\n",
        "\n",
        "    if current_player == 1:\n",
        "      move = min_max_agent(state, player=1, depth_limit=4)\n",
        "    else:\n",
        "      move = random_player(state, player=1)\n",
        "\n",
        "    if move is None:\n",
        "      break\n",
        "\n",
        "    state = result_state(state, move)\n",
        "  return utility(state)\n",
        "\n",
        "\n",
        "games = 1000\n",
        "results = {1: 0, -1: 0, 0: 0}\n",
        "for _ in range(games):\n",
        "  winner = play_time(board_shape=(4,4), depth_limit=4)\n",
        "  results[winner]+=1\n",
        "\n",
        "print(f'For {games} games')\n",
        "print(f'player 1:  won {results[1]} times')\n",
        "print(f'player -1: won {results[-1]} times')\n",
        "print(f'player resulted in at tie:  {results[0]} times')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXE2i58iL8co"
      },
      "source": [
        "## Task 4: Heuristic Alpha-Beta Tree Search [3 points]\n",
        "\n",
        "### Heuristic evaluation function\n",
        "\n",
        "Define and implement a heuristic evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qoCQIXw_L8co"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "1) make moves in the center of the board\n",
        "2) avoid that the opponent wins (identify the potential connect-4)\n",
        "3) try to add disks to places where winning is possible given the current board\n",
        "\n",
        "heuristic function: get a score of of chances of winning : high (more likely to win ex: 3 of player's disk) to low (least likely to win or thread ex: opponent 3 disks)\n",
        "'''\n",
        "\n",
        "def get_score(window_space, player, connect=4):\n",
        "  score= 0\n",
        "\n",
        "  #advanatege\n",
        "  # 4 in a row\n",
        "  if window_space.count(player) == connect:\n",
        "    return 100\n",
        "  # 3 in a row but space for one\n",
        "  elif window_space.count(player) == connect - 1 and window_space.count(0) == 1:\n",
        "    score+=70\n",
        "  #2 in a row but space for two\n",
        "  elif window_space.count(player) == connect - 2 and window_space.count(0) == 2:\n",
        "    score+=50\n",
        "\n",
        "\n",
        "  #opponent thread\n",
        "  if window_space.count(-player) == connect - 1 and window_space.count(0) == 1:\n",
        "    score-=55\n",
        "  elif window_space.count(-player) == connect - 2 and window_space.count(0) == 2:\n",
        "    score-=25\n",
        "  return score\n",
        "\n",
        "def heuristic(state, connect=4):\n",
        "\n",
        "  board, mean_flag, player = state\n",
        "  rows, columns = board.shape\n",
        "  score = 0\n",
        "\n",
        "  #advantage moves in the center\n",
        "  center = columns // 2\n",
        "  center_pos = list(board[:, center])\n",
        "  center_player_disk = center_pos.count(player)\n",
        "  score += center_player_disk * 2\n",
        "\n",
        "  #horizontal\n",
        "  for r in range(rows):\n",
        "    for c in range(columns - connect + 1):\n",
        "      window_space = list(board[r, c : c+ connect])\n",
        "      score += get_score(window_space, player)\n",
        "  #vertical\n",
        "  for c in range(columns):\n",
        "    for r in range(rows - connect + 1):\n",
        "      window_space = list(board[r : r+ connect, c])\n",
        "      score += get_score(window_space, player)\n",
        "  #diagonal \\\n",
        "  for r in range(connect - 1, rows):\n",
        "    for c in range(columns - connect + 1):\n",
        "      window_space = list([board[r - i, c + i] for i in range(connect)])\n",
        "  #diagonal /\n",
        "  for r in range(rows - connect + 1):\n",
        "    for c in range(columns - connect + 1):\n",
        "      window_space = list([board[r + i, c + i] for i in range(connect)])\n",
        "  score = max(min(score, 100), -100)\n",
        "  return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbIviqR6L8co"
      },
      "source": [
        "### Cutting off search\n",
        "\n",
        "Modify your Minimax Search with Alpha-Beta Pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IhKCwDTPL8co",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "88538ca8-765b-4bcf-90e4-6c7a52eba345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    (r,c)  move selected  time (s)\n",
            "0  (6, 4)  (standard, 0)  0.641659\n",
            "1  (6, 5)  (standard, 0)  1.170766\n",
            "2  (6, 6)  (standard, 0)  3.882929\n",
            "3  (6, 7)  (standard, 0)  0.013358\n",
            "4  (6, 8)  (standard, 0)  0.018073\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def min_max_value_heuristic(state, alpha, beta, depth, max_player):\n",
        "  #alpha: best exlpored option for the minimizer\n",
        "  #beta: best explored option for the maximizer\n",
        "  #depth: search depth\n",
        "  #max_player: original player maximizer\n",
        "  board, mean_flag, player = state\n",
        "\n",
        "  #if the game is over of depth is 0\n",
        "  if terminal(state) or depth == 0:\n",
        "    return heuristic(state)\n",
        "\n",
        "  #maximizing player (original player)\n",
        "  if state[2] == max_player:\n",
        "    value = float('-inf')\n",
        "    for action in actions(state):\n",
        "      child_state = result_state(state, action)\n",
        "      value = max(value, min_max_value_heuristic(child_state, alpha, beta, depth - 1, max_player))\n",
        "      alpha = max(alpha, value)\n",
        "      if alpha >= beta:\n",
        "        break\n",
        "    return value\n",
        "  #minimzing player (opponent player)\n",
        "  else:\n",
        "    value = float('inf')\n",
        "    for action in actions(state):\n",
        "      child_state = result_state(state, action)\n",
        "      value = min(value, min_max_value_heuristic(child_state, alpha, beta, depth - 1, max_player))\n",
        "      alpha = min(beta, value)\n",
        "      if beta <= alpha:\n",
        "        break\n",
        "    return value\n",
        "def min_max_agent_heuristic(state, player=None, depth_limit=4):\n",
        "  if player is None:\n",
        "    board, mean_flag, _ = state\n",
        "    state = (board, mean_flag, player)\n",
        "\n",
        "  max_player = state[2]\n",
        "  best_action = None\n",
        "  best_value = float('-inf')\n",
        "\n",
        "  for action in actions(state):\n",
        "    child_state = result_state(state, action)\n",
        "    value = min_max_value_heuristic(child_state, float('-inf'), float('inf'), depth_limit - 1, max_player)\n",
        "\n",
        "    if value > best_value:\n",
        "      best_value = value\n",
        "      best_action = action\n",
        "  return best_action\n",
        "def play_time_heuristic( board_shape=(4,4), depth_limit=4):\n",
        "  board = empty_board(board_shape)\n",
        "  state = (board, False, 1)\n",
        "\n",
        "  while not terminal(state):\n",
        "    current_player = state[2]\n",
        "\n",
        "    if current_player == 1:\n",
        "      move = min_max_agent_heuristic(state, player=1, depth_limit=4)\n",
        "    else:\n",
        "      move = random_player(state, player=1)\n",
        "\n",
        "    if move is None:\n",
        "      break\n",
        "\n",
        "    state = result_state(state, move)\n",
        "  return utility(state)\n",
        "\n",
        "\n",
        "games = 10\n",
        "results = {1: 0, -1: 0, 0: 0}\n",
        "\n",
        "depths = []\n",
        "total_time = 0\n",
        "for depth in [2,3,4,5,6]:\n",
        "  for _ in range(games):\n",
        "    start = time.time()\n",
        "\n",
        "    winner = play_time_heuristic(board_shape=(4,4), depth_limit=depth)\n",
        "    results[winner]+=1\n",
        "    total_time += time.time() - start\n",
        "\n",
        "\n",
        "    depths.append(depth)\n",
        "\n",
        "\n",
        "\n",
        "data = {\n",
        "    'depth': depths,\n",
        "    'wins': results[1],\n",
        "    'losses': results[-1],\n",
        "    'ties': results[0],\n",
        "    'average time (s)': total_time / games\n",
        "}\n",
        "delattrf = pd.DataFrame.from_dict(data)\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBpYGvxBL8co"
      },
      "source": [
        "Experiment with the same manually created boards as above to check if the agent spots winning opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KHqb_K5YL8co",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a071a106-e386-40d8-be70-40b1d0e0580c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    (r,c)  move selected  time (s)\n",
            "0  (6, 4)  (standard, 2)  0.361115\n",
            "1  (6, 5)  (standard, 2)  0.792177\n",
            "2  (6, 6)  (standard, 2)  2.021097\n",
            "3  (6, 7)  (standard, 0)  0.005434\n",
            "4  (6, 8)  (standard, 0)  0.007448\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "legal_moves =[]\n",
        "diamentions =[]\n",
        "for c in [4,5,6,7,8]:\n",
        "  board = winning_board(6, c, player=1)\n",
        "  state = (board, False, 1)\n",
        "  start = time.time()\n",
        "  move = min_max_agent_heuristic(state, player=1, depth_limit=4)\n",
        "  end = time.time() - start\n",
        "\n",
        "  times.append(end)\n",
        "  legal_moves.append(move)\n",
        "  diamentions.append((6,c))\n",
        "\n",
        "data = {\n",
        "  '(r,c)': diamentions,\n",
        "  'move selected': legal_moves,\n",
        "  'time (s)': times\n",
        "}\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaBC7W7VL8cp"
      },
      "source": [
        "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yb303vWbL8cp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7b208843-cb6a-4966-aa87-aad3a8447cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     (r,c)  move selected   time (s)\n",
            "0   (6, 4)  (standard, 4)  10.051220\n",
            "1   (6, 5)  (standard, 4)   8.868629\n",
            "2   (6, 6)  (standard, 4)  10.081222\n",
            "3   (6, 7)  (standard, 4)  10.178112\n",
            "4   (6, 8)  (standard, 4)   9.615595\n",
            "5   (6, 9)  (standard, 4)   9.251347\n",
            "6  (6, 10)  (standard, 4)  10.176538\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "legal_moves =[]\n",
        "diamentions =[]\n",
        "for c in [4,5,6,7,8,9,10]:\n",
        "  board = empty_board((6,col))\n",
        "  state = (board, False, 1)\n",
        "  start = time.time()\n",
        "  move = min_max_agent_heuristic(state, player=1, depth_limit=4)\n",
        "  end = time.time() - start\n",
        "\n",
        "  times.append(end)\n",
        "  legal_moves.append(move)\n",
        "  diamentions.append((6,c))\n",
        "\n",
        "data = {\n",
        "  '(r,c)': diamentions,\n",
        "  'move selected': legal_moves,\n",
        "  'time (s)': times\n",
        "}\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bokyrXrkL8cp"
      },
      "source": [
        "### Playtime\n",
        "\n",
        "Let two heuristic search agents (different cutoff depth, different heuristic evaluation function) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hUILjLbdL8cp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bbaaacee-abf5-42e3-cf0d-dd263e81a7fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   number of columns     game 1    game 2\n",
            "0                  4   0.334993  0.233907\n",
            "1                  5   0.846688  0.860843\n",
            "2                  6   2.349667  2.360590\n",
            "3                  7   4.910870  5.012482\n",
            "4                  8  10.019488  9.800241\n"
          ]
        }
      ],
      "source": [
        "def display_results():\n",
        "  columns =[]\n",
        "  game_1 = []\n",
        "  game_2 = []\n",
        "  trials = 10\n",
        "  for col in [4,5,6,7,8]:\n",
        "    board = empty_board((6,col))\n",
        "    state = (board, False, 1)\n",
        "\n",
        "    time_game_1 =[]\n",
        "    for _ in range(trials):\n",
        "      start = time.time()\n",
        "      move = min_max_agent_heuristic(state, player=1, depth_limit=4)\n",
        "      end = time.time() - start\n",
        "      time_game_1.append(end)\n",
        "\n",
        "    avg_time_1 = sum(time_game_1) / len(time_game_1)\n",
        "\n",
        "    #ordering min max agent\n",
        "    time_game_2 = []\n",
        "    for _ in range(trials):\n",
        "      start = time.time()\n",
        "      move = min_max_agent_heuristic(state, player=1, depth_limit=4)\n",
        "      end = time.time() - start\n",
        "      time_game_2.append(end)\n",
        "\n",
        "    avg_time_2 = sum(time_game_2) / len(time_game_2)\n",
        "\n",
        "    columns.append(col)\n",
        "    game_1.append(avg_time_1)\n",
        "    game_2.append(avg_time_2)\n",
        "\n",
        "  data = {\n",
        "        'number of columns': columns,\n",
        "        'game 1': game_1,\n",
        "        'game 2': game_2\n",
        "    }\n",
        "  df = pd.DataFrame.from_dict(data)\n",
        "  print(df)\n",
        "display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFhDH9M_L8cp"
      },
      "source": [
        "---\n",
        "Assignment adapted from [Michael Hahsler](https://github.com/mhahsler/CS7320-AI) under [CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/deed.en) license.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}